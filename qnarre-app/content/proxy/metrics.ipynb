{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representative Metrics Through Statistical Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: expand bullets\n",
    "\n",
    "- Meta-Data: From black & white to full color\n",
    "- Probabilistic Modeling: \"Quantumized\" credibility of claims\n",
    "- Learned Derivatives: coherence, fragmentation and turmoil\n",
    "- Profiling lawyers and predicting trends in Courtroom\n",
    "\n",
    "- more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have already too much accumulated code to load into our notebook. For your refence, we sample from the attached local source files: *datasets.py, layers.py, main.py, modules.py, samples.py, tasks.py, trafo.py* and *utils.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tuple(' ')\n",
    "metas = vocab + tuple('XYS OPS RES'.split())\n",
    "metas += tuple('ABCDEFGHIJ')\n",
    "separs = tuple(',;[]|')\n",
    "vocab += separs\n",
    "vocab += tuple('xy=$+-*')\n",
    "vocab += tuple('0123456789')\n",
    "masks = tuple('?_')\n",
    "vocab += masks\n",
    "\n",
    "tokens = {c: i for i, c in enumerate(vocab)}\n",
    "tokens.update((c[0], i) for i, c in enumerate(metas))\n",
    "\n",
    "SPC = tokens[vocab[0]]\n",
    "assert SPC == 0\n",
    "EOS = tokens[separs[-1]]\n",
    "MSK = tokens[masks[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tuple('grp enc dec tgt emt dmt out'.split())\n",
    "\n",
    "GRP, ENC, DEC, TGT, EMT, DMT, OUT = features\n",
    "\n",
    "def sampler(ps, groups):\n",
    "    def to_meta(g, x):\n",
    "        # print('to_metas x', x)\n",
    "        g = chr(ord('A') + g)\n",
    "        m, y = '', x.split(';')\n",
    "        if len(y) > 1:\n",
    "            m = 'X' * (len(y[0]) + 1)\n",
    "            y = y[1:]\n",
    "        y = y[0].split('[')\n",
    "        y2 = y[0].split('|')\n",
    "        m += 'O' * len(y2[0])\n",
    "        if len(y2) > 1:\n",
    "            m += g * (len(y2[1]) + 1)\n",
    "        if len(y) > 1:\n",
    "            y2 = y[1].split('|')\n",
    "            m += 'R' * (len(y2[0]) + 1)\n",
    "            if len(y2) > 1:\n",
    "                m += g * (len(y2[1]) + 1)\n",
    "        assert len(x) == len(m)\n",
    "        # print('to_metas m', m)\n",
    "        return m\n",
    "\n",
    "    for s in qs.sampler(ps):\n",
    "\n",
    "        def to_features(g):\n",
    "            fs = s[g]\n",
    "            g = qs.groups.index(g)\n",
    "            e, d, t, o = fs[ENC], fs[DEC], fs[TGT], fs.get(OUT, '')\n",
    "            d2 = t if '?' in d else d\n",
    "            return [f'#{g}', e, d, t, to_meta(g, e), to_meta(g, d2), o]\n",
    "\n",
    "        yield [to_features(g) for g in groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have mentioned already that we are largely reusing the model, specifically its layers and modules, from the first section of our blogs.\n",
    "\n",
    "In this blog we provide more details of the necessary changes in the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import layers as ql\n",
    "\n",
    "ks = tf.keras\n",
    "\n",
    "class Locate(ql.Layer):\n",
    "    span, spot = None, None\n",
    "\n",
    "    def __init__(self, ps, group):\n",
    "        super().__init__(ps)\n",
    "        h = self.cfg.dim_hidden\n",
    "        self.width = w = self.cfg.width_dec\n",
    "        if group is qs.QAS:\n",
    "            self.span = qm.Dense(self, 'span', [h * w, 2 * w])\n",
    "        else:\n",
    "            assert group is qs.FIX\n",
    "            self.spot = qm.Dense(self, 'spot', [h * w, w])\n",
    "\n",
    "    def cfg_items(self, ps):\n",
    "        yield from super().cfg_items(ps)\n",
    "        yield from ps.cfg_items(\n",
    "            'dim_hidden',\n",
    "            'width_dec',\n",
    "        )\n",
    "\n",
    "    @tf.function(input_signature=[[\n",
    "        tf.TensorSpec(shape=[None, None, None]),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.int32)\n",
    "    ]])\n",
    "    def call(self, x):\n",
    "        y, _ = x\n",
    "        s = tf.shape(y)\n",
    "        y = tf.reshape(y, [s[0], 1, -1])\n",
    "        if self.span is not None:\n",
    "            y = self.span(y)\n",
    "            y = tf.reshape(y, [s[0], 2, -1])\n",
    "        elif self.spot is not None:\n",
    "            y = self.spot(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as qu\n",
    "\n",
    "class Attention(tf.Module):\n",
    "    out = None\n",
    "\n",
    "    def __init__(self, layer, name):\n",
    "        super().__init__(name)\n",
    "        self.layer = layer\n",
    "        cfg = layer.cfg\n",
    "        h = cfg.dim_hidden\n",
    "        k = cfg.dim_attn_qk or cfg.dim_attn or h\n",
    "        self.scale = 1 / (k**0.5)\n",
    "        self.num_heads = n = cfg.num_heads or 1\n",
    "        v = cfg.dim_attn_v\n",
    "        if not v:\n",
    "            assert h % n == 0\n",
    "            v = h // n\n",
    "        self.drop_rate = cfg.drop_attn or cfg.drop_hidden\n",
    "        with self.name_scope:\n",
    "            self.q = Dense(layer, 'q', [h, n * k])\n",
    "            self.k = Dense(layer, 'k', [h, n * k])\n",
    "            self.v = Dense(layer, 'v', [h, n * v])\n",
    "            if n * v != h:\n",
    "                self.out = Dense(layer, 'out', [n * v, h])\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        inp, lens, ctx = x\n",
    "        off = tf.math.reduce_max(lens)\n",
    "        x = inp[:, -off:, :]\n",
    "        q = self.split_heads(self.q(x))\n",
    "        k = self.split_heads(self.k(ctx))\n",
    "        v = self.split_heads(self.v(ctx))\n",
    "        y = tf.einsum('bnxi,bnci->bnxc', q, k)\n",
    "        y *= self.scale\n",
    "        # use lens\n",
    "        y = tf.nn.softmax(y)\n",
    "        y = tf.einsum('bnxc,bnci->bnxi', y, v)\n",
    "        y = self.join_heads(y)\n",
    "        if self.out is not None:\n",
    "            y = self.out(y)\n",
    "        y = self.layer.drop(y, self.drop_rate)\n",
    "        y = self.layer.norm(x + y)\n",
    "        y = tf.concat([inp[:, :-off, :], y], axis=1)\n",
    "        return [y, lens]\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        s = tf.shape(x)\n",
    "        y = tf.reshape(x, [s[0], s[1], self.num_heads, -1])\n",
    "        y = tf.transpose(y, perm=[0, 2, 1, 3])\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def join_heads(x):\n",
    "        y = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        s = tf.shape(y)\n",
    "        y = tf.reshape(y, [s[0], s[1], -1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(tf.Module):\n",
    "    epsilon = 1e-3\n",
    "\n",
    "    def __init__(self, layer, name, shape):\n",
    "        super().__init__(name)\n",
    "        kw = dict(shape=shape, dtype=tf.float32)\n",
    "        with self.name_scope:\n",
    "            self.gamma = layer.add_weight('gamma', initializer='ones', **kw)\n",
    "            self.beta = layer.add_weight('beta', initializer='zeros', **kw)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, None])])\n",
    "    def __call__(self, x):\n",
    "        mean, variance = tf.nn.moments(x, -1, keepdims=True)\n",
    "        kw = dict(offset=self.beta,\n",
    "                  scale=self.gamma,\n",
    "                  variance_epsilon=self.epsilon)\n",
    "        y = tf.nn.batch_normalization(x, mean, variance, **kw)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our blog, please click on the next blog for further details of the additional changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.qpy)",
   "language": "python",
   "name": "qpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
