{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograph: Intuitive Data-Driven Control At Last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing our blogs, we shift our focus to yet another new feature in TF, the `autograph` functionality.\n",
    "\n",
    "Previously, and as far as intuitive expression of code was concerned, \"graph ops\" efficiently solved complex calculations while failed at simple, sequential control.\n",
    "\n",
    "By generating on-demand Python code now, `autograph` transparently patches all the necessary graph ops together and packages the result into a \"python op\".\n",
    "\n",
    "While the generated new ops are potentially faster than the code before them, in this blog we are more interested in the new expressive powers of the `autograph` package.\n",
    "\n",
    "Specifically, we look at what becomes possible when decorating our functions with the new `tf.function` decorator, as doing this would by default invoke the `autograph` functionality.\n",
    "\n",
    "Our objective is to ultimately arrive at a model as represented by the [graph](./autograph.pdf).\n",
    "\n",
    "Just as before, we need to prep our environment to run any meaningful code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dataset as qd\n",
    "import custom as qc\n",
    "import layers as ql\n",
    "ks = tf.keras\n",
    "kl = ks.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we borrow the `pos_timing` function from our previous blogs, and override it to return a constant \"timing signal\" tensor, depending on the `width` and `depth` arguments.\n",
    "\n",
    "As our first task is to implement a \"python branch\" in our new `Embed` op, we will be using two different \"timing\" tensors, one for the `encode` input and the other for the `decode` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_timing(width, depth):\n",
    "    t = ql.pos_timing(width, depth)\n",
    "    t = tf.constant(t, dtype=tf.float32)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Embed` layer will thus create the two constant tensors to be sourced in the subsequent `call` methods.\n",
    "\n",
    "Our model will call the shared `Embed` instance for both of our stacks. As we have decorated its `call` method with `tf.function`, we can use familiar and intuitive Python comparisons to branch on the value of tensors on-the-fly, during graph execution.\n",
    "\n",
    "Clearly, our two stacks, while having the same `depth`s, have different `width`s. Also the constant \"timing\" tensors have different `width`s as well.\n",
    "\n",
    "Yet we are still able to pick-and-match the otherwise incompatible tensors and successfully add them together, all depending on the actual `width` of our \"current\" input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(qc.Embed):\n",
    "    def __init__(self, ps):\n",
    "        super().__init__(ps)\n",
    "        self.enc_p = pos_timing(ps.width_enc, ps.dim_hidden)\n",
    "        self.dec_p = pos_timing(ps.width_dec, ps.dim_hidden)\n",
    "\n",
    "    @tf.function(input_signature=[[\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.int32)\n",
    "    ]])\n",
    "    def call(self, x):\n",
    "        y, lens = x\n",
    "        y = tf.nn.embedding_lookup(self.emb, y)\n",
    "        s = tf.shape(y)\n",
    "        if s[-2] == self.ps.width_enc:\n",
    "            y += tf.broadcast_to(self.enc_p, s)\n",
    "        elif s[-2] == self.ps.width_dec:\n",
    "            y += tf.broadcast_to(self.dec_p, s)\n",
    "        else:\n",
    "            pass\n",
    "        y *= tf.cast(s[-1], tf.float32)**0.5\n",
    "        return [y, lens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we demonstrate how on-the-fly \"python ops\" can also provide insights into inner processes and data flows.\n",
    "\n",
    "We borrow our `Frames` layer from the previous blog and override its `call` method with a `tf.function` decorated new version that, besides calling `super().call()`, also calls a new `print_row` Python function on every row in our batch.\n",
    "\n",
    "Yes, we are calling a Python function and printing its results in a TF graph op while never leaving our intuitive and familiar Python environment! Isn't that great?\n",
    "\n",
    "The `print_row` function itself is simple, it iterates through the tokens of the \"row\", it does a lookup of each in our `vocab` \"table\" for the actual character representing the token and then it \"joins\" all the characters and prints out the resulting string.\n",
    "\n",
    "And, if we scroll down to the listing of our training session, we can actually see the \"sliding context\" of our samples as they fly by during our training.\n",
    "\n",
    "Needless to say, the listing confirms that our `Frames` layer does a good job concatenating the varied length sample inputs, the target results, as well as the necessary separators.\n",
    "\n",
    "As a result, a simple Python function, usable during graph ops, provides us invaluable insights deep into our inner processes and data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frames(qc.Frames):\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        y = super().call.python_function(x)\n",
    "        tf.print()\n",
    "\n",
    "        def print_row(r):\n",
    "            tf.print(\n",
    "                tf.numpy_function(\n",
    "                    lambda ts: ''.join([qd.vocab[t] for t in ts]),\n",
    "                    [r],\n",
    "                    Tout=[tf.string],\n",
    "                ))\n",
    "            return r\n",
    "\n",
    "        tf.map_fn(print_row, self.prev)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next new layer is the partial `Deduce` layer, showcasing how control is intuitive at last from data-driven branching to searching.\n",
    "\n",
    "This layer will be used in the next group of blogs as a replacement for our previous `Debed` layer. It contains a tensor-dependent `for` loop to iteratively replace our masked characters with \"deduced\" ones.\n",
    "\n",
    "The future `Probe` layer, building on the `Deduce` scheme, implements an approximation of \"Beam Search\", see [paper](https://arxiv.org/pdf/1702.01806.pdf).\n",
    "\n",
    "It effectively iterates through the hidden dimensions of the output, and based on parallel `topk` searches, comparing various choices for \"debeding\" the output, it settles on an \"optimal\" debedding and thus final token output for our `decoder`.\n",
    "\n",
    "Without `autograph` the data-driven looping/branching graph ops would have to be expressed in a much more convoluted manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Deduce(Layer):\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        toks, *x = x\n",
    "        if self.cfg.runtime.print_toks:\n",
    "            qu.print_toks(toks, qd.vocab)\n",
    "        y = self.deduce([toks] + x)\n",
    "        n = tf.shape(y)[1]\n",
    "        p = tf.shape(toks)[1] - n\n",
    "        for i in tf.range(n):\n",
    "            t = toks[:, :n]\n",
    "            m = tf.equal(t, qd.MSK)\n",
    "            if tf.equal(tf.reduce_any(m), True):\n",
    "                t = self.update(t, m, y)\n",
    "                if self.cfg.runtime.print_toks:\n",
    "                    qu.print_toks(t, qd.vocab)\n",
    "                toks = tf.pad(t, [[0, 0], [0, p]])\n",
    "                y = self.deduce([toks] + x)\n",
    "            else:\n",
    "                e = tf.equal(t, qd.EOS)\n",
    "                e = tf.math.count_nonzero(e, axis=1)\n",
    "                if tf.equal(tf.reduce_any(tf.not_equal(e, 1)), False):\n",
    "                    break\n",
    "        return y\n",
    "\"\"\"\n",
    "class Probe(ql.Layer):\n",
    "    def __init__(self, ps):\n",
    "        super().__init__(ps)\n",
    "        self.dbd = qc.Dense(self, 'dbd', [ps.dim_hidden, ps.dim_vocab])\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        y, lens = x\n",
    "        s = tf.shape(y)\n",
    "        y = tf.reshape(y, [s[0] * s[1], -1])\n",
    "        y = self.dbd(y)\n",
    "        y = tf.reshape(y, [s[0], s[1], -1])\n",
    "        y = y[:, :tf.math.reduce_max(lens), :]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model needs to be updated as well to use the newly defined components.\n",
    "\n",
    "Other than that, we are ready to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for(ps):\n",
    "    x = [ks.Input(shape=(), dtype='int32'), ks.Input(shape=(), dtype='int64')]\n",
    "    x += [ks.Input(shape=(), dtype='int32'), ks.Input(shape=(), dtype='int64')]\n",
    "    x += [ks.Input(shape=(), dtype='int32'), ks.Input(shape=(), dtype='int64')]\n",
    "    y = qc.ToRagged()(x)\n",
    "    y = Frames(ps)(y)\n",
    "    embed = Embed(ps)\n",
    "    ye = qc.Encode(ps)(embed(y[:2]))\n",
    "    yd = qc.Decode(ps)(embed(y[2:]) + [ye[0]])\n",
    "    y = Probe(ps)(yd)\n",
    "    m = ks.Model(inputs=x, outputs=y)\n",
    "    m.compile(optimizer=ps.optimizer, loss=ps.loss, metrics=[ps.metric])\n",
    "    print(m.summary())\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By firing up our training session, we can confirm the model's layers and connections. The listing of a short session follows.\n",
    "\n",
    "We can easily adjust the parameters to tailor the length of the sessions to our objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "to_ragged_3 (ToRagged)          [(None, None), (None 0           input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "frames_3 (Frames)               [(5, 25), (None,), ( 125         to_ragged_3[0][0]                \n",
      "                                                                 to_ragged_3[0][1]                \n",
      "                                                                 to_ragged_3[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "embed_3 (Embed)                 multiple             120         frames_3[0][0]                   \n",
      "                                                                 frames_3[0][1]                   \n",
      "                                                                 frames_3[0][2]                   \n",
      "                                                                 frames_3[0][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "encode_3 (Encode)               [(None, 25, 6), (Non 90516       embed_3[0][0]                    \n",
      "                                                                 embed_3[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decode_3 (Decode)               [(None, 15, 6), (Non 54732       embed_3[1][0]                    \n",
      "                                                                 embed_3[1][1]                    \n",
      "                                                                 encode_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "probe_3 (Probe)                 (None, None, None)   140         decode_3[0][0]                   \n",
      "                                                                 decode_3[0][1]                   \n",
      "==================================================================================================\n",
      "Total params: 145,633\n",
      "Trainable params: 145,508\n",
      "Non-trainable params: 125\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "[\"        y=76,x=34:y-x:42|\"]\n",
      "[\"       y=12,x=33:x*y:396|\"]\n",
      "[\"     y=-13,x=-80:x-y:-67|\"]\n",
      "[\"      x=51,y=-70:x-y:121|\"]\n",
      "[\"       y=-24,x=30:x-y:54|\"]\n",
      "      1/Unknown - 3s 3s/step - loss: 3.0014 - sparse_categorical_crossentropy: 3.0014\n",
      "[\"x:42|x=-15,y=-38:y*x:570|\"]\n",
      "[\"x*y:396|x=36,y=72:y-x:36|\"]\n",
      "[\"y:-67|x=-93,y=55:y+x:-38|\"]\n",
      "[\"-y:121|x=2,y=-66:y-x:-68|\"]\n",
      "[\"x-y:54|x=-1,y=-59:x*y:59|\"]\n",
      "      2/Unknown - 3s 1s/step - loss: 2.9652 - sparse_categorical_crossentropy: 2.9652\n",
      "[\"570|y=59,x=-78:x*y:-4602|\"]\n",
      "[\":36|y=-98,x=-78:y*x:7644|\"]\n",
      "[\":-38|x=-21,y=-36:y+x:-57|\"]\n",
      "[\"y-x:-68|x=21,y=40:y+x:61|\"]\n",
      "[\"x*y:59|y=31,x=-12:x+y:19|\"]\n",
      "      3/Unknown - 4s 1s/step - loss: 2.9888 - sparse_categorical_crossentropy: 2.9956\n",
      "[\"y:-4602|y=59,x=66:y-x:-7|\"]\n",
      "[\":7644|y=21,x=67:x*y:1407|\"]\n",
      "[\"x:-57|x=-51,y=-69:x-y:18|\"]\n",
      "[\":61|y=49,x=-70:y*x:-3430|\"]\n",
      "[\"x+y:19|y=53,x=15:x*y:795|\"]\n",
      "      4/Unknown - 5s 1s/step - loss: 2.9879 - sparse_categorical_crossentropy: 2.9924\n",
      "[\":y-x:-7|y=52,x=50:x-y:-2|\"]\n",
      "[\"1407|y=-86,x=40:y-x:-126|\"]\n",
      "[\"-y:18|x=48,y=-43:y-x:-91|\"]\n",
      "[\":-3430|x=99,y=-24:x+y:75|\"]\n",
      "[\"795|x=94,y=-79:x*y:-7426|\"]\n",
      "      5/Unknown - 6s 1s/step - loss: 2.9691 - sparse_categorical_crossentropy: 2.9697\n",
      "[\"y:-2|x=17,y=-37:x*y:-629|\"]\n",
      "[\"126|x=99,y=-94:y*x:-9306|\"]\n",
      "[\"x:-91|y=-82,x=63:x+y:-19|\"]\n",
      "[\":75|x=-51,y=-79:x*y:4029|\"]\n",
      "[\"426|y=-67,x=-44:x*y:2948|\"]\n",
      "      6/Unknown - 7s 1s/step - loss: 2.9654 - sparse_categorical_crossentropy: 2.9654\n",
      "[\"y:-629|y=72,x=28:y+x:100|\"]\n",
      "[\"306|y=93,x=-67:x*y:-6231|\"]\n",
      "[\"-19|y=83,x=-61:y*x:-5063|\"]\n",
      "[\"4029|x=-19,y=-63:x+y:-82|\"]\n",
      "[\":2948|y=-5,x=-31:y*x:155|\"]\n",
      "      7/Unknown - 8s 1s/step - loss: 2.9354 - sparse_categorical_crossentropy: 2.9323\n",
      "[\"x:100|x=42,y=83:x*y:3486|\"]\n",
      "[\":-6231|x=-8,y=23:x-y:-31|\"]\n",
      "[\"*x:-5063|x=7,y=40:y+x:47|\"]\n",
      "[\"-82|y=-63,x=-35:y*x:2205|\"]\n",
      "[\"155|y=-68,x=-17:y*x:1156|\"]\n",
      "      8/Unknown - 9s 1s/step - loss: 2.9268 - sparse_categorical_crossentropy: 2.9247\n",
      "[\":3486|x=97,y=30:y*x:2910|\"]\n",
      "[\"y:-31|y=-50,x=-71:y-x:21|\"]\n",
      "[\"y+x:47|x=44,y=59:x+y:103|\"]\n",
      "[\"*x:2205|x=23,y=66:y-x:43|\"]\n",
      "[\"1156|y=-90,x=76:y-x:-166|\"]\n",
      "      9/Unknown - 10s 1s/step - loss: 2.8991 - sparse_categorical_crossentropy: 2.8989\n",
      "[\":2910|x=-20,y=72:x-y:-92|\"]\n",
      "[\"y-x:21|y=-1,x=91:y-x:-92|\"]\n",
      "[\"+y:103|x=-14,y=0:y+x:-14|\"]\n",
      "[\":43|x=-78,y=64:y*x:-4992|\"]\n",
      "[\"166|x=-81,y=16:y*x:-1296|\"]\n",
      "10/10 [==============================] - 11s 1s/step - loss: 2.8581 - sparse_categorical_crossentropy: 2.8533\n"
     ]
    }
   ],
   "source": [
    "ps = qd.Params(**qc.params)\n",
    "ps.num_epochs = 1\n",
    "import masking as qm\n",
    "qm.main_graph(ps, qc.dset_for(ps).take(10), model_for(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our TensorBoard `callback` in place, the model's `fit` method will generate the standard summaries that TB can conveniently visualize.\n",
    "\n",
    "If you haven't run the code below, an already generated graph is [here](./autograph.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir /tmp/q/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our blog, please see how to use customize the losses and metrics driving the training by clicking on the next blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
