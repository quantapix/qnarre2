{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: The Tensor \"Highway\" On-Ramp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is about lots and lots of data. Organizing the input data is an error-prone, arduous task. \n",
    "\n",
    "TensorFlow `datasets` were designed to build complex input data pipelines from simple, reusable pieces with the clear objective of normalizing that process.\n",
    "\n",
    "This blog shows off some of the useful features of this new approach to \"feed the beast\".\n",
    "\n",
    "Before we can run any meaningful code, we first need to prep our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib as pth\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, and brevity, let's create some aliases as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = tf.data\n",
    "tt = tf.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While computers were made to add numbers together, they quickly run into insurmountable obstacles if these numbers are presented as simple textual sequences of digits.\n",
    "\n",
    "We aim to finally \"teach\" our computer to correctly add and multiply, just as we learned in elementary school. And we start with a simple yet fascinating example, inspired by https://arxiv.org/pdf/1812.02825.pdf.\n",
    "\n",
    "Our input data consists of `num_samples` (perhaps easily millions) of `\"x=-12,y=24:y+x:12\"`-like strings, or lines, of texts. These visibly consist of `defs`, `op` and `res` fields (separated by `:`).\n",
    "\n",
    "Our variables are: `x` and `y`, our \"operations\" are: `=`, `+`, `-` and `*`, and our variables can be assigned values from: `[-max_val, max_val]`.\n",
    "\n",
    "The rest of the blogs in this group will continue to build on the below presented results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = (' ', ':', '|')\n",
    "vocab += ('x', 'y', '=', ',', '+', '-', '*')\n",
    "vocab += ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also intend to make our input data pipeline parametric.\n",
    "\n",
    "However, the obvious simple and intuitive Python `dict` structures, with literal string keys, are error-prone exactly because of unchecked literals.\n",
    "\n",
    "A few lines of code gives as the `Params` class that leverages the native Python attribute mechanism to validate the names of all our params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    max_val=10,\n",
    "    num_samples=4,\n",
    "    num_shards=3,\n",
    ")\n",
    "\n",
    "class Params:\n",
    "    def __init__(self, **kw):\n",
    "        for k, v in kw.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now randomly generate our data, fittingly as a Python generator, and based on a given `Params` instance. For this we define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_gen(ps):\n",
    "    m, n = ps.max_val, ps.num_samples\n",
    "    # x, y vals in defs\n",
    "    vals = np.random.randint(low=1 - m, high=m, size=(2, n))\n",
    "    # (x, y) order if 1 in defs [0] and op [1], respectively\n",
    "    ords = np.random.randint(2, size=(2, n))\n",
    "    # index of ['+', '-', '*']\n",
    "    ops = np.array(['+', '-', '*'])\n",
    "    ops.reshape((1, 3))\n",
    "    ops = ops[np.random.randint(3, size=n)]\n",
    "    for i in range(n):\n",
    "        x, y = vals[:, i]\n",
    "        res = f'x={x},y={y}:' if ords[0, i] else f'y={y},x={x}:'\n",
    "        o = ops[i]\n",
    "        res += (f'x{o}y:' if ords[1, i] else f'y{o}x:')\n",
    "        if o == '+':\n",
    "            res += f'{x + y}'\n",
    "        elif o == '*':\n",
    "            res += f'{x * y}'\n",
    "        else:\n",
    "            assert o == '-'\n",
    "            res += (f'{x - y}' if ords[1, i] else f'{y - x}')\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our generator defined, it takes just a line of code to create millions of correct \"exercises\" or samples for our training sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=-4,x=4:y-x:-8\n",
      "y=4,x=-8:y*x:-32\n",
      "y=-8,x=7:y-x:-15\n",
      "y=-4,x=9:y+x:5\n"
     ]
    }
   ],
   "source": [
    "ps = Params(**params)\n",
    "for s in py_gen(ps):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.data.Dataset` class is the main abstraction for our sequence of elements.\n",
    "\n",
    "Each element of a dataset is one or more Tensors containing the fields, or `features`, of our `sample` \"lines\" of elementary math exercises.\n",
    "\n",
    "Using our \"in-memory\" generator, we can directly create a TF dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_src(ps):\n",
    "    ds = td.Dataset.from_generator(\n",
    "        lambda: py_gen(ps),\n",
    "        tf.string,\n",
    "        tf.TensorShape([]),\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the first 2 samples of the now tensor-based sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'x=-6,y=0:x+y:-6', shape=(), dtype=string)\n",
      "tf.Tensor(b'y=1,x=7:y+x:8', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dg = gen_src(ps)\n",
    "for s in dg.take(2):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input data pipeline starts with a \"source\" dataset, perhaps just as simple as the above.\n",
    "\n",
    "This \"source\" can also be a `range`, `from_tensor_slices`, `from_tensors` and even a `TextLineDataset` (see the TF docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_dset(ps):\n",
    "    ds = np.array(list(py_gen(ps)))\n",
    "    ds = td.Dataset.from_tensor_slices(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets can then be consumed one-by-one as iterables or as aggregatables (e.g. using reduce ops) collections.\n",
    "\n",
    "Datasets also allow chaining of handy \"transformations\" to themselves. Some of the canned operations are the intuitive: *cache, concatenate, enumerate, reduce, repeat, shuffle, skip, take, zip*.\n",
    "\n",
    "An example of 2 samples of a new dataset, concatenated with all 4 samples of the previous, gen-based, dataset and also \"enumerated\" on-the-fly is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(b'x=9,y=-9:y+x:0', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(b'x=-4,y=2:y+x:-2', shape=(), dtype=string)\n",
      "tf.Tensor(2, shape=(), dtype=int64) tf.Tensor(b'x=5,y=-7:y-x:-12', shape=(), dtype=string)\n",
      "tf.Tensor(3, shape=(), dtype=int64) tf.Tensor(b'y=-4,x=1:x+y:-3', shape=(), dtype=string)\n",
      "tf.Tensor(4, shape=(), dtype=int64) tf.Tensor(b'y=2,x=4:x-y:2', shape=(), dtype=string)\n",
      "tf.Tensor(5, shape=(), dtype=int64) tf.Tensor(b'y=-6,x=-5:y-x:-1', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ds = src_dset(ps)\n",
    "for i, s in ds.take(2).concatenate(dg).enumerate():\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter our to be \"pipeline\" at any stage, with the objective of perhaps dropping unfit samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering \"x=9,y=-9:y+x:0\"... in\n",
      "0 tf.Tensor(b'x=9,y=-9:y+x:0', shape=(), dtype=string)\n",
      "filtering \"x=-4,y=2:y+x:-2\"... out\n",
      "filtering \"x=-2,y=3:x*y:-6\"... out\n",
      "filtering \"x=-3,y=5:y-x:8\"... in\n",
      "1 tf.Tensor(b'x=-3,y=5:y-x:8', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def filterer(x):\n",
    "    r = tf.strings.length(x) < 15\n",
    "    tf.print(tf.strings.format('filtering {}... ', x) + ('in' if r else 'out'))\n",
    "    return r\n",
    "\n",
    "for i, s in enumerate(ds.filter(filterer)):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, we can split the pipeline into named \"filaments\" of data.\n",
    "\n",
    "This new feature proves to be extremely useful, allowing us to standardize and unify all our data sources with configurable, on-the-fly channeling of features aggregated therein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'defs': <tf.Tensor: id=207, shape=(), dtype=string, numpy=b'x=9,y=-9'>, 'op': <tf.Tensor: id=208, shape=(), dtype=string, numpy=b'y+x'>, 'res': <tf.Tensor: id=209, shape=(), dtype=string, numpy=b'0'>}\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def splitter(x):\n",
    "    fs = tf.strings.split(x, ':')\n",
    "    return {'defs': fs[0], 'op': fs[1], 'res': fs[2]}\n",
    "\n",
    "for s in ds.map(splitter).take(1):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a \"pipeline component\" is an in-line Python `dict`-based tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'defs': <tf.Tensor: id=258, shape=(8,), dtype=int32, numpy=array([ 3,  5, 19,  6,  4,  5,  8, 19], dtype=int32)>, 'op': <tf.Tensor: id=259, shape=(3,), dtype=int32, numpy=array([4, 7, 3], dtype=int32)>, 'res': <tf.Tensor: id=260, shape=(1,), dtype=int32, numpy=array([10], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "tokens = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "@tf.function\n",
    "def tokenizer(d):\n",
    "    return {\n",
    "        k: tf.numpy_function(\n",
    "            lambda x: tf.constant([tokens[chr(c)] for c in x]),\n",
    "            [v],\n",
    "            Tout=tf.int32,\n",
    "        )\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "\n",
    "for s in ds.map(splitter).map(tokenizer).take(1):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be potentially very large, fitting only on disk and in many files.\n",
    "\n",
    "As transparent data-pipeline performance is key for training throughput, datasets can also be efficiently encoded into binary sequences stored in `sharded` files.\n",
    "\n",
    "The following will convert our samples into such binary \"records\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def records(dset):\n",
    "    for s in dset:\n",
    "        fs = tt.Features(\n",
    "            feature={\n",
    "                'defs': tt.Feature(int64_list=tt.Int64List(value=s['defs'])),\n",
    "                'op': tt.Feature(int64_list=tt.Int64List(value=s['op'])),\n",
    "                'res': tt.Feature(int64_list=tt.Int64List(value=s['res'])),\n",
    "            })\n",
    "        yield tt.Example(features=fs).SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can \"dump\" our tokenized, ready-to-consume samples into shards of files stored in a directory.\n",
    "\n",
    "Once these prepared samples are stored, we can \"stream\" them straight into our models without any more prep (see subsequent blogs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping /tmp/q/dataset/shard_0000.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0001.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0002.tfrecords...\n"
     ]
    }
   ],
   "source": [
    "def shards(ps):\n",
    "    for _ in range(ps.num_shards):\n",
    "        yield src_dset(ps).map(splitter).map(tokenizer)\n",
    "\n",
    "def dump(ps):\n",
    "    d = pth.Path('/tmp/q/dataset')\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    for i, ds in enumerate(shards(ps)):\n",
    "        i = '{:0>4d}'.format(i)\n",
    "        p = str(d / f'shard_{i}.tfrecords')\n",
    "        print(f'dumping {p}...')\n",
    "        with tf.io.TFRecordWriter(p) as w:\n",
    "            for r in records(ds):\n",
    "                w.write(r)\n",
    "        yield p\n",
    "        \n",
    "fs = [f for f in dump(ps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For streaming, or loading the \"records\" back, we need to create templates used in interpreting the stored binary data.\n",
    "\n",
    "With the templates defined, loading them back in, straight into our datasets, can be just as follows.\n",
    "\n",
    "Note that the names of the shard files are conveniently returned by our \"dump\" function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'defs': tf.io.VarLenFeature(tf.int64),\n",
    "    'op': tf.io.VarLenFeature(tf.int64),\n",
    "    'res': tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "\n",
    "def load(ps, files):\n",
    "    ds = td.TFRecordDataset(files)\n",
    "    if ps.dim_batch:\n",
    "        ds = ds.batch(ps.dim_batch)\n",
    "        return ds.map(lambda x: tf.io.parse_example(x, features))\n",
    "    return ds.map(lambda x: tf.io.parse_single_example(x, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually start using the loaded data in our models, let's \"adapt\" the pipeline to supply dense tensors instead of the originally configured sparse ones.\n",
    "\n",
    "Also, since we haven't batched anything yet, we set `dim_batch` to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "1 3\n",
      "2 3\n",
      "3 3\n",
      "4 3\n",
      "5 3\n",
      "6 3\n",
      "7 3\n",
      "8 3\n",
      "9 3\n",
      "10 3\n",
      "11 3\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def adapter(d):\n",
    "    return [\n",
    "        tf.sparse.to_dense(d['defs']),\n",
    "        tf.sparse.to_dense(d['op']),\n",
    "        tf.sparse.to_dense(d['res']),\n",
    "    ]\n",
    "\n",
    "ps.dim_batch = None\n",
    "for i, s in enumerate(load(ps, fs).map(adapter)):\n",
    "    print(i, len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listing above reveals that we merged 3 sharded files, worth 4 samples each, into the 12 printed samples. We only printed the number of features for each sample, for brevity.\n",
    "\n",
    "**Please also note** how the above in-line adapter converted our named features into unnamed, positional, i.e. in-a-list features. This was necessary as the Keras `Input` doesn't recognize named input tensors yet.\n",
    "\n",
    "If we turn on batching in our dataset, the same code will now return the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (<tf.Tensor: id=1362, shape=(2, 8), dtype=int64, numpy=\n",
      "array([[ 4,  5, 14,  6,  3,  5,  8, 11],\n",
      "       [ 3,  5, 11,  6,  4,  5,  8, 12]])>, <tf.Tensor: id=1363, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[4, 7, 3],\n",
      "       [4, 8, 3]])>, <tf.Tensor: id=1364, shape=(2, 2), dtype=int64, numpy=\n",
      "array([[13,  0],\n",
      "       [ 8, 13]])>)\n",
      "1 (<tf.Tensor: id=1368, shape=(2, 9), dtype=int64, numpy=\n",
      "array([[ 4,  5,  8, 16,  6,  3,  5,  8, 16],\n",
      "       [ 4,  5,  8, 12,  6,  3,  5,  8, 14]])>, <tf.Tensor: id=1369, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[3, 8, 4],\n",
      "       [3, 9, 4]])>, <tf.Tensor: id=1370, shape=(2, 1), dtype=int64, numpy=\n",
      "array([[10],\n",
      "       [18]])>)\n",
      "2 (<tf.Tensor: id=1374, shape=(2, 8), dtype=int64, numpy=\n",
      "array([[ 3,  5,  8, 11,  6,  4,  5, 10],\n",
      "       [ 3,  5, 12,  6,  4,  5, 14,  0]])>, <tf.Tensor: id=1375, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[4, 7, 3],\n",
      "       [4, 8, 3]])>, <tf.Tensor: id=1376, shape=(2, 2), dtype=int64, numpy=\n",
      "array([[ 8, 11],\n",
      "       [12,  0]])>)\n",
      "3 (<tf.Tensor: id=1380, shape=(2, 8), dtype=int64, numpy=\n",
      "array([[ 4,  5, 15,  6,  3,  5,  8, 11],\n",
      "       [ 4,  5, 10,  6,  3,  5, 19,  0]])>, <tf.Tensor: id=1381, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[4, 9, 3],\n",
      "       [3, 7, 4]])>, <tf.Tensor: id=1382, shape=(2, 2), dtype=int64, numpy=\n",
      "array([[ 8, 15],\n",
      "       [19,  0]])>)\n",
      "4 (<tf.Tensor: id=1386, shape=(2, 8), dtype=int64, numpy=\n",
      "array([[ 4,  5, 12,  6,  3,  5, 10,  0],\n",
      "       [ 4,  5, 19,  6,  3,  5,  8, 17]])>, <tf.Tensor: id=1387, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[4, 8, 3],\n",
      "       [3, 9, 4]])>, <tf.Tensor: id=1388, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[12,  0,  0],\n",
      "       [ 8, 16, 13]])>)\n",
      "5 (<tf.Tensor: id=1392, shape=(2, 8), dtype=int64, numpy=\n",
      "array([[ 4,  5,  8, 13,  6,  3,  5, 17],\n",
      "       [ 4,  5, 10,  6,  3,  5, 14,  0]])>, <tf.Tensor: id=1393, shape=(2, 3), dtype=int64, numpy=\n",
      "array([[4, 7, 3],\n",
      "       [3, 9, 4]])>, <tf.Tensor: id=1394, shape=(2, 1), dtype=int64, numpy=\n",
      "array([[14],\n",
      "       [10]])>)\n"
     ]
    }
   ],
   "source": [
    "ps.dim_batch = 2\n",
    "for i, s in enumerate(load(ps, fs).map(adapter)):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As preparation for the subsequent blogs, let's generate a more substantial data source with 10 shards of 1,000 samples each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping /tmp/q/dataset/shard_0000.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0001.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0002.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0003.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0004.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0005.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0006.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0007.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0008.tfrecords...\n",
      "dumping /tmp/q/dataset/shard_0009.tfrecords...\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "ps.max_val = 100\n",
    "ps.num_samples = 1000\n",
    "ps.num_shards = 10\n",
    "fs = [f for f in dump(ps)]\n",
    "ps.dim_batch = 100\n",
    "for i, _ in enumerate(load(ps, fs).map(adapter)):\n",
    "    pass\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our blog, please see how easy masking our uneven sample \"lines\" can be by clicking on the next blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.qpy)",
   "language": "python",
   "name": "qpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
