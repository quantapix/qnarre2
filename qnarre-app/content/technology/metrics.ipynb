{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular And Reusable Metrics All The Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have run various training sessions, and we have approached modeling itself from various angles, it is time for us to consider the piece of TF functionality that ties it all together.\n",
    "\n",
    "Ultimately, the end goal of any modeling, and of the actual training with our models, is to arrive to simply quantifiable \"measures\", the `losses`, that we can then use in our repeated iterations of gradient calculations and subsequent tuning the weights of the model.\n",
    "\n",
    "A `loss` is defined as some sort of difference, or \"distance\", between the results of our model's calculations and the given targets.\n",
    "\n",
    "Specifically, we have been using `crossentropy`, (see a fun explanation [here](https://colah.github.io/posts/2015-09-Visual-Information/)) as that \"distance\".\n",
    "\n",
    "Just as before, we need to prep our environment to run any meaningful code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dataset as qd\n",
    "import custom as qc\n",
    "import autograph as qa\n",
    "ks = tf.keras\n",
    "kl = ks.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to experiment with multiple `loss` and `metrics` settings, we duplicate our `tgt` tensors in our newly defined `adapter` function of our `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def adapter(d):\n",
    "    enc, dec, tgt = d['enc'], d['dec'], d['tgt']\n",
    "    return ((\n",
    "        enc.flat_values,\n",
    "        enc.row_splits,\n",
    "        dec.flat_values,\n",
    "        dec.row_splits,\n",
    "        tgt.flat_values,\n",
    "        tgt.row_splits,\n",
    "    ), (\n",
    "        tgt.to_tensor(),\n",
    "        tgt.to_tensor(),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also adjust our `ToRagged` layer.\n",
    "\n",
    "Instead of leaving the `tf.function` decorator generic, which is allowing multiple version of the op to be generated based on the actual shapes of the input tensors, we restrict the generated op to only one version: the one taking triple input tensor pairs of any 1D shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToRagged(qc.ToRagged):\n",
    "    @tf.function(input_signature=[[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.int64)\n",
    "    ] * 3])\n",
    "    def call(self, x):\n",
    "        ys = []\n",
    "        for i in range(3):\n",
    "            i *= 2\n",
    "            fv, rs = x[i:i + 2]\n",
    "            ys.append(tf.RaggedTensor.from_row_splits(fv, rs))\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to tackle replacement `loss` and `metric` classes.\n",
    "\n",
    "The Keras `losses.Loss` base class, that all the various other \"losses\" are derived from, has a `call` method with the above mentioned two arguments: the target tensor and the model's output tensor.\n",
    "\n",
    "Our replacement implementation of the method skips the various checks and validations from the canned version and simply flattens the two known tensors followed by calling directly the efficient graph op implementation of crossentropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ks.losses.Loss):\n",
    "    @staticmethod\n",
    "    def xent(tgt, out):\n",
    "        tgt = tf.reshape(tf.cast(tgt, tf.int64), [-1])\n",
    "        s = tf.shape(out)\n",
    "        out = tf.reshape(out, [-1, s[-1]])\n",
    "        y = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tgt,\n",
    "                                                           logits=out)\n",
    "        return tf.reshape(y, s[:-1])\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name='loss')\n",
    "\n",
    "    def call(self, tgt, out):\n",
    "        return self.xent(tgt, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `Metric` class is even simpler, it adds the aggregating `total` and `count` variables and then delegates to calling our `xent` function (the same that our matching `Loss` uses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(ks.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='metric', dtype=tf.float32)\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, tgt, out, sample_weight=None):\n",
    "        vs = Loss.xent(tgt, out)\n",
    "        self.total.assign_add(tf.math.reduce_sum(vs))\n",
    "        return self.count.assign_add(tf.cast(tf.size(vs), dtype=tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total, self.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model needs to be updated to use the newly defined components, including our new `Loss` and `Metric` classes.\n",
    "\n",
    "As we include both the `Debed` and the `Probe` layers from our previous blogs, and they show up as a pair of output tensors respectively identifiable by their names, we can assign different losses and metrics to each.\n",
    "\n",
    "We chose to use the same `loss` and `metric` for both. Keras, as expected, will sum both losses and metrics to calculate the end result.\n",
    "\n",
    "To keep things simple, and since we have a pair of outputs, we also had to double the targets that the `loss` and `metric` would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for(ps):\n",
    "    x = [ks.Input(shape=(), dtype='int32'), ks.Input(shape=(), dtype='int64')]\n",
    "    x += [ks.Input(shape=(), dtype='int32'), ks.Input(shape=(), dtype='int64')]\n",
    "    x += [ks.Input(shape=(), dtype='int32'), ks.Input(shape=(), dtype='int64')]\n",
    "    y = ToRagged()(x)\n",
    "    y = qc.Frames(ps)(y)\n",
    "    embed = qc.Embed(ps)\n",
    "    ye = qc.Encode(ps)(embed(y[:2]))\n",
    "    yd = qc.Decode(ps)(embed(y[2:]) + [ye[0]])\n",
    "    y = qc.Debed(ps)(yd)\n",
    "    ys = qa.Probe(ps)(yd)\n",
    "    m = ks.Model(inputs=x, outputs=[y, ys])\n",
    "    m.compile(\n",
    "        optimizer=ps.optimizer,\n",
    "        loss={'debed': ps.loss, 'probe': ps.loss},\n",
    "        metrics={'debed': [ps.metric], 'probe': [ps.metric]},\n",
    "    )\n",
    "    print(m.summary())\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to update our parameters with instances of our specific `Loss` and `Metric` instances we want Keras to call on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = qc.params\n",
    "params.update(\n",
    "    loss=Loss(),\n",
    "    metric=Metric(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to start our training session.\n",
    "\n",
    "We can confirm the model's layers and connections and we can easily adjust the parameters to tailor the length of the sessions to our objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "to_ragged (ToRagged)            [(None, None), (None 0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "frames (Frames)                 [(5, 25), (None,), ( 125         to_ragged[0][0]                  \n",
      "                                                                 to_ragged[0][1]                  \n",
      "                                                                 to_ragged[0][2]                  \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embed)                   multiple             120         frames[0][0]                     \n",
      "                                                                 frames[0][1]                     \n",
      "                                                                 frames[0][2]                     \n",
      "                                                                 frames[0][3]                     \n",
      "__________________________________________________________________________________________________\n",
      "encode (Encode)                 [(None, 25, 6), (Non 90516       embed[0][0]                      \n",
      "                                                                 embed[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "decode (Decode)                 [(None, 15, 6), (Non 54732       embed[1][0]                      \n",
      "                                                                 embed[1][1]                      \n",
      "                                                                 encode[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "debed (Debed)                   (None, None, None)   140         decode[0][0]                     \n",
      "                                                                 decode[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "probe (Probe)                   (None, None, None)   140         decode[0][0]                     \n",
      "                                                                 decode[0][1]                     \n",
      "==================================================================================================\n",
      "Total params: 145,773\n",
      "Trainable params: 145,648\n",
      "Non-trainable params: 125\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "10/10 [==============================] - 11s 1s/step - loss: 5.8626 - debed_loss: 2.9345 - probe_loss: 2.9280 - probe_metric: 2.9239\n"
     ]
    }
   ],
   "source": [
    "ps = qd.Params(**params)\n",
    "ps.num_epochs = 1\n",
    "import masking as qm\n",
    "qm.main_graph(ps, qc.dset_for(ps, adapter).take(10), model_for(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our TensorBoard `callback` in place, the model's `fit` method will generate the standard summaries that TB can conveniently visualize.\n",
    "\n",
    "If you haven't run the code below, an already generated graph is [here](./metrics.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir /tmp/q/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our blog, please see how to use Keras callbacks by clicking on the next blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
