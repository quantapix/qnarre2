{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predatory Profits Of \"High-Conflict\" Divorces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a justification of this blog, please consider the following chain of conjectures (referenced \"lawyers\" would be the **divorce lawyer** types):\n",
    "- our common law is adversarial, the winner simply takes all\n",
    "- a family, by definition, is not adversarial, as fundamentally both parents love their children\n",
    "- however, with no adversarial conflict, there is no lawsuit and no profits for lawyers\n",
    "- thus our common law applied to families must first turn a family into adversaries\n",
    "- by definition, either unsolved conflicts or perceived lack of resources create adversaries\n",
    "- moreover, sustainably intractable conflicts guarantee adversaries for life or \"high-conflict\"\n",
    "- however, with no money, i.e. no possible profits for lawyers, there simply cannot be \"high-conflicts\"\n",
    "- \"high-conflict\" cases thus are an ambitious, i.e. ruthless, lawyer's \"gold mines\" and  job-security\n",
    "- lawyers are in overabundance and competition is fierce, as one only needs to be a malicious actor\n",
    "- however, with no \"high-conflict\", there are no trendsetting, \"interesting\" cases\n",
    "- with no trendsetting, there is no [~$500 / hour billing rate](https://femfas.net/flip_burgers/index.html) for ruthless, and narcissist, \"top lawyers\"\n",
    "\n",
    "Accepting the above chain of faultless logic, what can a deeply narcissist divorce lawyer do?\n",
    "- in cases lacking conflicts, he has only one choice: provoke or **flat-out fabricate a conflict by blatantly lying**, specifically for the Family Court's eager consumption\n",
    "- if he \"leaves money on the table\", and neglects exploiting lucrative cases he has already hooked-onto, **he will go hungry** with everyone watching!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog we focus on *directly* fabricated conflicts, or flat-out, knowingly stated lies to the Family Courts by our lawyers. We are aided by the strict rules of the Court, as all meaningful communication must already be in (or should be convertible to) textual English \"inputs\".\n",
    "\n",
    "Our first goal is to train our computer to \"catch the knowingly and directly lying lawyer\" by systematically finding direct, irrefutable textual contradictions in *all* of a lawyer's communications.\n",
    "\n",
    "Current state-of-the-art NLP research (see [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf)) has shown that the various proposed mechanism for answering generic semantic correctness questions are exceedingly promising. We use them to train our elementary arithmetic model in telling us if a simple mathematical expression is correct or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we have too much accumulated code to load into our notebook. For your refence, we sample from the attached local source files: *datasets.py, layers.py, main.py, modules.py, samples.py, tasks.py, trafo.py* and *utils.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Samples` class randomly generates samples from an on-demand allocated pool of values. The size of the pool is set by the `dim_pool` param and using it with large values helps with keeping the probability distributions in check.\n",
    "\n",
    "Currently, `Samples` can generate a variety of 10 different groups of samples. In this blog we focus on `yes-no` (YNS), `masked` (MSK), `reversed` (REV) and `faulty` (FIX) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple sample generating loop can be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import samples as qs\n",
    "\n",
    "groups = tuple('yns ynx msk msx cls clx qas rev gen fix'.split())\n",
    "\n",
    "YNS, YNX, MSK, MSX, CLS, CLX, QAS, REV, GEN, FIX = groups\n",
    "\n",
    "def sampler(ps):\n",
    "    ss = qs.Samples(ps)\n",
    "    for _ in range(ps.num_samples):\n",
    "        ss, idx = ss.next_idx\n",
    "        enc, res, *_ = ss.create(idx)\n",
    "        dec = tgt = f'[{res}]'\n",
    "        bad = f'[{ss.other(res)}]'\n",
    "        yn = ss.yns[0, idx]\n",
    "\n",
    "        d2 = dec if yn else bad\n",
    "        yns = dict(enc=enc, dec=d2 + '|_', tgt=d2 + f'|{yn}')\n",
    "\n",
    "        yield {YNS: yns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated samples are Python `dict`s with the previously introduced `enc` (encoder), `dec` (decoder) and `tgt` (target) features.\n",
    "\n",
    "Both `dec` and `tgt` features end the sample with `|` and the yes-no answer is encoded as `1` and `0` (the `_` is the place-holder that the decoder needs to solve).\n",
    "\n",
    "And now we can generate a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enc': 'x=81,y=11;x+y', 'dec': '[10]|_', 'tgt': '[10]|0'}\n",
      "{'enc': 'y=-99,x=-58;x+y', 'dec': '[-157]|_', 'tgt': '[-157]|1'}\n",
      "{'enc': 'x=13,y=-79;y-x', 'dec': '[-92]|_', 'tgt': '[-92]|1'}\n",
      "{'enc': 'y=-33,x=-30;y+x', 'dec': '[-96]|_', 'tgt': '[-96]|0'}\n"
     ]
    }
   ],
   "source": [
    "import utils as qu\n",
    "\n",
    "ps = dict(\n",
    "    dim_pool=3,\n",
    "    max_val=100,\n",
    "    num_samples=4,\n",
    ")\n",
    "ps = qu.Params(**ps)\n",
    "\n",
    "for d in sampler(ps):\n",
    "    print(f'{d[YNS]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we don't show any of the other samples in this blog, the `MSK` features mask the results at random positions with a `?`, the `REV` samples mix up the order of the variables and `FIX` samples randomly introduce an error digit in the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual model is largely similar to the models already presented in the previous blogs.\n",
    "\n",
    "Based on what group of samples we are using, we activate some layers in the model while ignoring others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant consideration is that all the 10 groups of samples contribute (if meaningful) to the same weights (or variables).\n",
    "\n",
    "We chose to do this this based on the results of the [MT-DNN](https://arxiv.org/pdf/1901.11504.pdf) paper. Varying the type and challenge of the samples we effectively cross-train the model.\n",
    "\n",
    "In order to clearly separate the `loss` and `metric` calculations between the groups, we create a new instance of our model for each group of samples. However, we reuse the same layers.\n",
    "\n",
    "To accomplish this, we define an `lru_cache` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=32)\n",
    "def layer_for(cls, *pa, **kw):\n",
    "    return cls(*pa, **kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, our usual `model_for` function looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for(ps, group):\n",
    "    x = inputs\n",
    "    y = layer_for(ql.ToRagged)(x)\n",
    "    yt = layer_for(ql.Tokens, ps)(y)\n",
    "    ym = layer_for(ql.Metas, ps)(y)\n",
    "    xe, xd = yt[:2] + ym[:1], yt[2:] + ym[1:]\n",
    "    embed = layer_for(ql.Embed, ps)\n",
    "    ye = layer_for(ql.Encode, ps)(embed(xe))[0]\n",
    "    decode = layer_for(ql.Decode, ps)\n",
    "    if group in (qs.YNS, qs.YNX):\n",
    "        y = decode(embed(xd) + [ye])\n",
    "        y = layer_for(ql.Debed, ps)(y)\n",
    "    elif group in (qs.MSK, qs.MSX):\n",
    "        y = layer_for(ql.Deduce, ps, embed, decode)(xd + [ye])\n",
    "    if group in (qs.QAS, qs.FIX):\n",
    "        y = decode(embed(xd) + [ye])\n",
    "        y = layer_for(ql.Locate, ps, group)(y)\n",
    "    m = Model(name='trafo', inputs=x, outputs=[y])\n",
    "    m.compile(optimizer=ps.optimizer, loss=ps.loss, metrics=[ps.metric])\n",
    "    print(m.summary())\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have expanded the functionality of our layers and modules from the previous blogs, our params have increased in number.\n",
    "\n",
    "Also, the subsequent blogs in this section will describe the additions to the model's extended functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datasets as qd\n",
    "ks = tf.keras\n",
    "\n",
    "params = dict(\n",
    "    activ_concl='gelu',\n",
    "    dim_attn=4,\n",
    "    dim_attn_qk=None,\n",
    "    dim_attn_v=None,\n",
    "    dim_batch=5,\n",
    "    dim_concl=150,\n",
    "    dim_hidden=6,\n",
    "    dim_hist=5,\n",
    "    dim_metas=len(qd.metas),\n",
    "    dim_stacks=2,\n",
    "    dim_vocab=len(qd.vocab),\n",
    "    drop_attn=None,\n",
    "    drop_concl=None,\n",
    "    drop_hidden=0.1,\n",
    "    initer_stddev=0.02,\n",
    "    loss=ks.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metric=ks.metrics.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    num_epochs=2,\n",
    "    num_heads=3,\n",
    "    num_rounds=2,\n",
    "    num_shards=2,\n",
    "    optimizer=ks.optimizers.Adam(),\n",
    "    width_dec=40,\n",
    "    width_enc=50,\n",
    ")\n",
    "\n",
    "params.update(\n",
    "    loss=qu.Loss(),\n",
    "    metric=qu.Metric(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our new `main` function that loops through all the samples in all of our groups of samples and either trains the model on the samples or performs an evaluation/prediction.\n",
    "\n",
    "In the follow-on blogs we present the various training/eval/predict functions that our main loop can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ps, fn, groups=None, count=None):\n",
    "    qu.Config.runtime.is_training = True\n",
    "    groups = groups or qs.groups\n",
    "    for r in range(ps.num_rounds):\n",
    "        for g in groups:\n",
    "            print(f'\\nRound {r + 1}, group {g}...\\n=======================')\n",
    "            fn(ps, qd.dset_for(ps, g, count=count), model_for(ps, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start a training session, we need to generate some samples.\n",
    "\n",
    "The code that generates the samples is similar to the following.\n",
    "\n",
    "The `large` datasets generate 100 shards containing each 10,000 samples for every every sample group out of the current 10.\n",
    "\n",
    "The total number of samples for the `large` dataset can be easily varied, however, with the pictured settings, it amounts to **10 million samples** that a server with 40 hyper-threads generates in about 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_small = dict(\n",
    "    dim_batch=5,\n",
    "    dim_pool=10,\n",
    "    max_val=1000,\n",
    "    num_samples=20,\n",
    "    num_shards=2,\n",
    ")\n",
    "\n",
    "ds_large = dict(\n",
    "    dim_batch=1000,\n",
    "    dim_pool=1024 * 1024,\n",
    "    max_val=100000,\n",
    "    num_samples=10000,\n",
    "    num_shards=100,\n",
    ")\n",
    "\n",
    "def dump_ds(kind):\n",
    "    ps = qu.Params(**(ds_small if kind == 'small' else ds_large))\n",
    "    ss = [s for s in qd.dump(ps, f'/tmp/q/data/{kind}')]\n",
    "    ds = qd.load(ps, shards=ss).map(qd.adapter)\n",
    "    for i, _ in enumerate(ds):\n",
    "        pass\n",
    "    print(f'dumped {i + 1} batches of {ps.dim_batch} samples each')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is an actual call to generate our `small` sample set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping /tmp/q/data/small/cls/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/msk/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/yns/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/qas/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/clx/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/msx/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/ynx/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/rev/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/gen/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/yns/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/msk/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/cls/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/fix/shard_0000.tfrecords...\n",
      "dumping /tmp/q/data/small/ynx/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/clx/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/msx/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/qas/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/gen/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/rev/shard_0001.tfrecords...\n",
      "dumping /tmp/q/data/small/fix/shard_0001.tfrecords...\n",
      "dumped 80 batches of 5 samples each\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run a short training session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1, group yns...\n",
      "=======================\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 9s 4s/step - loss: 3.2370 - metric: 3.2364\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.2212 - metric: 3.2209\n",
      "\n",
      "Round 1, group msk...\n",
      "=======================\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 32s 16s/step - loss: 3.2135 - metric: 3.2134\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 3.2034 - metric: 3.2032\n",
      "\n",
      "Round 1, group qas...\n",
      "=======================\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 7s 4s/step - loss: 3.4434 - metric: 3.4434\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.7450 - metric: 2.7450\n",
      "\n",
      "Round 2, group yns...\n",
      "=======================\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 7s 4s/step - loss: 3.2059 - metric: 3.2070\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.1923 - metric: 3.1935\n",
      "\n",
      "Round 2, group msk...\n",
      "=======================\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 29s 14s/step - loss: 3.1887 - metric: 3.1887\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 3.1745 - metric: 3.1744\n",
      "\n",
      "Round 2, group qas...\n",
      "=======================\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 10s 5s/step - loss: 1.9412 - metric: 1.9412\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.3604 - metric: 1.3604\n"
     ]
    }
   ],
   "source": [
    "!python trafo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this concludes our blog, please click on the next blog for more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.qpy)",
   "language": "python",
   "name": "qpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
